p8105_hw2_rk3445
================
Rosie Kwon

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.2     ✔ tibble    3.3.0
    ## ✔ lubridate 1.9.4     ✔ tidyr     1.3.1
    ## ✔ purrr     1.1.0     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(readxl)
```

## Problem 1

**First, import and clean the data in pols-month.csv. Use `separate()`
to break up the variable `mon` into integer variables `year`, `month`,
and `day`; create a `president` variable taking values `gop` and `dem`,
and remove `prez_dem` and `prez_gop`; and remove the day variable.**

``` r
pols_month = 
  read_csv("data/pols-month.csv") |> 
  janitor::clean_names() |> 
  separate(
    mon, into = c("year", "month", "day"), sep = "-") |> 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    president = case_when(
      prez_gop == 1 ~ "gop",
      prez_dem == 1 ~ "dem")) |> 
  select(year, month, president, everything(), -c(prez_dem, prez_gop, day))
```

    ## Rows: 822 Columns: 9
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl  (8): prez_gop, gov_gop, sen_gop, rep_gop, prez_dem, gov_dem, sen_dem, r...
    ## date (1): mon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

**Second, clean the data in snp.csv using a similar process to the
above. For consistency across datasets, arrange according to year and
month, and organize so that year and month are the leading columns.**

``` r
snp = 
  read_csv("data/snp.csv") |> 
  janitor::clean_names() |> 
  separate(
    date, into = c("month", "day", "year"), sep = "/") |> 
  mutate(
    year = as.integer(year),
    year = ifelse(year < 50, 2000 + year, 1900 + year),
    month = as.integer(month)
  ) |> 
  select(year, month, everything(), -day) |> 
  arrange(year, month)
```

    ## Rows: 787 Columns: 2
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (1): date
    ## dbl (1): close
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

**Third, tidy the unemployment data so that it can be merged with the
previous datasets. This process will involve switching from “wide” to
“long” format; ensuring that key variables have the same name; and
ensuring that key variables take the same values.**

``` r
unemploy = 
  read_csv("data/unemployment.csv") |> 
  janitor::clean_names() |> 
  pivot_longer(
    cols = jan:dec,
    names_to = 'month',
    values_to = 'percentage'
  ) |> 
  mutate(
    month = match(month, tolower(month.abb)),
    year = as.integer(year)
  )
```

    ## Rows: 68 Columns: 13
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (13): Year, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

**Join the datasets by merging snp into pols, and merging unemployment
into the result.**

``` r
merge_df = 
  left_join(snp, pols_month, by = c("year", "month")) |> 
  left_join(unemploy, by = c("year", "month"))
```

**Write a short paragraph about these datasets. Explain briefly what
each dataset contained, and describe the resulting dataset (e.g. give
the dimension, range of years, and names of key variables).**

The `pols_month` dataset has 822 rows of observations and 9 variables,
recording the political composition of national politicians from 1947 to
2015. It includes indicators of whether the president was Republican or
Democratic (`president`), as well as counts of Republican and Democratic
governors (`gov_gop`, `gov_dem`), senators (`sen_gop`, `sen_dem`), and
representatives (`rep_gop`, `rep_dem`) at each point in time. The `snp`
dataset contains daily closing prices of the S&P 500, with 787
observations from 1950 to 2015. The `unemploy` dataset contains monthly
percentage of unemployment from 1948 to 2015.

These three datasets were merged using `left_join()` with `snp` dataset
as the reference. The merge was performed on the year and month
variables, the resulting dataset allows analysis of the relationship
between political composition, unemployment indicators, and stock market
performance. The merged dataset spans the years 1950 to 2015.

## Problem 2

**Read and clean the Mr. Trash Wheel sheet: specify the sheet in the
Excel file and to omit non-data entries (rows with notes / figures;
columns containing notes) using arguments in read_excel use reasonable
variable names. omit rows that do not include dumpster-specific data
round the number of sports balls to the nearest integer and converts the
result to an integer variable (using as.integer)**

``` r
mr_trash = 
  read_excel(
    "data/202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Mr. Trash Wheel", 
    skip = 1,
    na = c("NA", "")) |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |>
  select(dumpster:homes_powered) |> 
  mutate(
    year = as.double(year),
    sports_balls = as.integer(round(sports_balls))
  ) 
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

**Use a similar process to import, clean, and organize the data for
Professor Trash Wheel and Gwynnda.**

Professor Trash Wheel dataset

``` r
professor_trash = 
  read_excel(
    "data/202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Professor Trash Wheel", 
    skip = 1,
    na = c("NA","")) |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster), !is.na(date)) 
```

Gwynnda Trash Wheel dataset

``` r
gwynnda_trash = 
  read_excel(
    "data/202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Gwynnda Trash Wheel", 
    skip = 1,
    na = c("NA","")) |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) 
```

Combine this with the Mr. Trash Wheel dataset to produce a single tidy
dataset. To keep track of which Trash Wheel is which, you may need to
add an additional variable to both datasets before combining.

``` r
merged_trash =
  bind_rows(
    mr_trash |>  mutate(wheel = "mr"),
    professor_trash |>  mutate(wheel = "professor"),
    gwynnda_trash |>  mutate(wheel = "gwynnda")
  ) |> 
  select(wheel, everything())
```

**Write a paragraph about these data; you are encouraged to use inline
R. Be sure to note the number of observations in the resulting dataset,
and give examples of key variables. For available data, what was the
total weight of trash collected by Professor Trash Wheel? What was the
total number of cigarette butts collected by Gwynnda in June of 2022?**

The `mr_trash` dataset has 651 dumpsters recorded with 14 different
variables describing the collection. Across all three wheels,the data
include measures such as total trash weight (`weight_tons`), the number
of discarded cigarette butts (`cigarette_butts`), polystyrene
(`polystyrene`), plastic bottles (`plastic_bottles`), glass bottles
(`glass_bottles`), plastic bags (`plastic_bags`), and wrappers
(`wrappers`). Only Mr. trash wheel collected the data of the number of
sports ball (`sports_balls`).

The `Professor Trash Wheel` recorded 118 dumpsters and total weight of
trash collected was 246.74 tons.

The `Gwynnda Trash Wheel` collected 263 dumpsters data collection and
collected the total number of cigarette butts was 18120 in June of 2022.

## Problem 3

**Create a single, well-organized dataset with all the information
contained in these data files. To that end: import, clean, tidy, and
otherwise wrangle each of these datasets; check for completeness and
correctness across datasets (e.g. by viewing individual datasets and
monitoring warning messages);**

Import, clean, tidy NYC zipcode dataset

``` r
zipcode = 
  read_csv("data/Zip Codes.csv") |> 
  janitor::clean_names()
```

    ## Rows: 322 Columns: 7
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (4): County, County Code, File Date, Neighborhood
    ## dbl (3): State FIPS, County FIPS, ZipCode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Import, clean, tidy Zillow Observed Rent Index (ZORI) in New York City
dataset

``` r
zori_nyc = 
  read_csv("data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv") |> 
  janitor::clean_names() |> 
  rename(
    zip_code = region_name,
  ) |> 
  mutate(
    county_name = str_remove(county_name, " County$")
  )
```

    ## Rows: 149 Columns: 125
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr   (6): RegionType, StateName, State, City, Metro, CountyName
    ## dbl (119): RegionID, SizeRank, RegionName, 2015-01-31, 2015-02-28, 2015-03-3...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

merge to create a single, final dataset; and organize this so that
variables and observations are in meaningful orders.

``` r
A = zori_nyc |> 
  select(region_name = zip_code, county_name)

B = zipcode |> 
  select(zip_code, county)

compare_county = 
  left_join(A, B, by = c("region_name" = "zip_code")) |> 
  mutate(
    county_clean = str_remove(county_name, " County$"),
    match = county_clean == county
  )
```

While both datasets contain county information, these labels are not
always consistent. Therefore, in the merged dataset, I used zip_code as
the unique join key between Zillow ZORI data and NYC ZIP code data.

``` r
merge_zori = 
  left_join(zori_nyc, zipcode, by = c("zip_code")) |> 
  select(county:neighborhood, everything())
```

**Briefly describe the resulting tidy dataset. How many total
observations exist? How many unique ZIP codes are included, and how many
unique neighborhoods?**

The merged dataset `merge_zori` combines rental price information from
the Zillow Observed Rent Index (ZORI) with demographic and geographic
details from the NYC Zip Code dataset. In total, it contains 151
observations with 149 unique zip codes and 43 unique neighborhoods. Each
observation includes the state FIPS code, county, borough, and
neighborhood, along with rental price estimates (ZORI) tracked over
time.

**Which ZIP codes appear in the ZIP code dataset but not in the Zillow
Rental Price dataset? Using a few illustrative examples discuss why
these ZIP codes might be excluded from the Zillow dataset.**

``` r
not_included = 
  anti_join(zipcode, zori_nyc, by = "zip_code") 
```

There are 171 ZIP codes that appear in the ZIP code dataset but do not
appear in the Zillow Rental Price dataset. Some of these ZIP codes might
correspond to commercial or industrial zones rather than residential
areas, meaning there would be little or no rental housing data
available. Also Zillow may lack sufficient listings or reliable rental
price estimates in those areas, especially if the residential population
is small or housing turnover is limited.

**Rental prices fluctuated dramatically during the COVID-19 pandemic.
For all available ZIP codes, compare rental prices in January 2021 to
prices in January 2020. Make a table that shows the 10 ZIP codes (along
with the borough and neighborhood) with largest drop in price from
January 2020 to 2021. Comment.**

``` r
covid_19 = merge_zori |> 
  select(zip_code, county, neighborhood, x2020_01_31, x2021_01_31) |> 
  mutate(
    drop = x2020_01_31 - x2021_01_31
  ) |> 
  arrange(desc(drop)) |> 
  slice(1:10) 
  

knitr::kable(covid_19)
```

| zip_code | county | neighborhood | x2020_01_31 | x2021_01_31 | drop |
|---:|:---|:---|---:|---:|---:|
| 10007 | New York | Lower Manhattan | 6334.211 | 5421.614 | 912.5966 |
| 10069 | New York | NA | 4623.042 | 3874.918 | 748.1245 |
| 10009 | New York | Lower East Side | 3406.442 | 2692.187 | 714.2550 |
| 10016 | New York | Gramercy Park and Murray Hill | 3731.135 | 3019.431 | 711.7045 |
| 10001 | New York | Chelsea and Clinton | 4108.098 | 3397.648 | 710.4499 |
| 10002 | New York | Lower East Side | 3645.416 | 2935.113 | 710.3028 |
| 10004 | New York | Lower Manhattan | 3149.658 | 2443.697 | 705.9608 |
| 10038 | New York | Lower Manhattan | 3573.201 | 2875.616 | 697.5853 |
| 10012 | New York | Greenwich Village and Soho | 3628.566 | 2942.344 | 686.2218 |
| 10010 | New York | Gramercy Park and Murray Hill | 3697.284 | 3012.353 | 684.9304 |

The 10 zipcopde with the largest drop in price during the COVID-19
pandemic are all located in New York county (Manhattan borough). The
maximum drop was about 913, observed in Lower Manhattan. The
neighborhoods represented include Lower Manhattan, Lower East Side,
Gramercy Park and Murray Hill, Chelsea and Clinton, Greenwich Village
and Soho.
